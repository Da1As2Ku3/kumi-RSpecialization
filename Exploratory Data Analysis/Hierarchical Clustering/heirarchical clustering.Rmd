---
title: "Hierarchical Clustering in R"
author: "David Asare Kumi"
date: "June 11, 2019"
output: html_document
---


##Hierarchical Clustering

This is for high dimensional data. Clustering organizes things that are close into groups.

###Usefulness

1. Useful for high dimensional data.

2. Organizes the data in a logical and intuitive way.

- How do we define close?

- How do we group things?

- How do we visualize the grouping?

- How do we interpret the grouping?

###Hierarchical Clustering

- Common approach. An agglomerative approach.

1. Find closest two things.

2. Put them together.

3. Find next closest.

- Requires

1. A defined distance.

2. A merging approach.

- Produces

1. A tree showing how close things are to each other.

##Example1
 
```{r,echo=TRUE,message=FALSE,warning=FALSE}
set.seed(1234)
par(mar=c(0,0,0,0))
x <-rnorm(12,mean=rep(1:3,each=4),sd=0.2)
y <-rnorm(12,mean=rep(c(1,2,1),each=4),sd=0.2)
plot(x,y,col="blue",cex=2,pch=20)
text(x+0.05,y+0.05,label=as.character(1:12))

```

##Hierarchical Clustering distance

- The first thing to do is to calculate the distance between all the points.

```{r,echo=TRUE,message=FALSE,warning=FALSE}
x <-rnorm(12,mean=rep(1:3,each=4),sd=0.2)
y <-rnorm(12,mean=rep(c(1,2,1),each=4),sd=0.2)
dataFrame <-data.frame(x=x,y=y)
#This calculates the distance between all the different rows and gives you the distance matrix.
dist(dataFrame)
hclustering <-hclust(dist(dataFrame))
plot(hclustering)

```

- The farther down the tree are points merged(clustered) together first and those up the tree are points merged(clustered) later.

- This actually does not tell you how many clusters you have. To resolve this, cut the tree at a certain point to determine how many clusters there are.

- If you cut the tree at 2, you get 2 clusters. If you cut the tree at 1, you get 3 clusters.

- It is important to note that we do not have a rule for cutting the tree.

##heatmap() function

- This runs a hierarchical analysis on the rows and columns of the table.

- Heatmap function is useful for quickly looking at table or matrix data.

```{r,echo=TRUE,message=FALSE,warning=FALSE}
x <-rnorm(12,mean=rep(1:3,each=4),sd=0.2)
y <-rnorm(12,mean=rep(c(1,2,1),each=4),sd=0.2)
dataFrame <-data.frame(x=x,y=y)
set.seed(123)
dataMatrix <-as.matrix(dataFrame)[sample(1:12),]
heatmap(dataMatrix)

```

##K-Means Clustering

```{r,echo=TRUE,message=FALSE,warning=FALSE}
set.seed(1234)
par(mar=c(0,0,0,0))
x <-rnorm(12,mean=rep(1:3,each=4),sd=0.2)
y <-rnorm(12,mean=rep(c(1,2,1),each=4),sd=0.2)
plot(x,y,col="blue",pch=20,cex=2)
text(x+0.05,y+0.05,labels=as.character(1:12))
#K-means clustering (part 2)
dataFrame <-data.frame(x,y)
kmeansObj <-kmeans(dataFrame,centers=3)
names(kmeansObj)
kmeansObj$cluster
kmeansObj$centers #This tells you the location of the centroids in the space
#Plot
par(mar=rep(0.2,4))
plot(x,y,col=kmeansObj$centers,pch=20,cex=2)
points(kmeansObj$centers,col=1:3,pch=3,cex=3,lwd=3)

```

##Heatmaps

We can also use heatmaps here.

```{r,echo=TRUE,message=FALSE,warning=FALSE}
set.seed(1234)
x <-rnorm(12,mean=rep(1:3,each=4),sd=0.2)
y <-rnorm(12,mean=rep(c(1,2,1),each=4),sd=0.2)
plot(x,y,col="blue",pch=20,cex=2)
text(x+0.05,y+0.05,labels=as.character(1:12))
dataFrame <-data.frame(x,y)
dataMatrix <-as.matrix(dataFrame)[sample(1:12),]
kmeansObj2 <-kmeans(dataMatrix,centers=3)
par(mfrow=c(1,2),mar=c(2,4,0.1,0.1))
image(t(dataMatrix)[,nrow(dataMatrix):1],yaxt="n")
image(t(dataMatrix)[,order(kmeansObj$cluster)],yaxt="n")

```

##Dimension Reduction (part1)

```{r,echo=TRUE,message=FALSE,warning=FALSE}
x <-rnorm(12,mean=rep(1:3,each=4),sd=0.2)
y <-rnorm(12,mean=rep(c(1,2,1),each=4),sd=0.2)
dataFrame <-data.frame(x,y)
dataMatrix <-as.matrix(dataFrame)[sample(1:12),]
par(mar=rep(0.2,4))
heatmap(dataMatrix)

#Matrix data
set.seed(12345)
par(mar=rep(0.2,4))
dataMatrix <-matrix(rnorm(400),nrow=40)
image(1:10,1:40,t(dataMatrix)[,nrow(dataMatrix):1])

#Run the heatmap function
heatmap(dataMatrix)

#What if we add a pattern
set.seed(678910)
for(i in 40){
  #flip a coin
  coinFlip <-rbinom(1,size=1,prob=0.5)
  #if coin is heads add a common pattern to the row
  if(coinFlip){
    dataMatrix[i,]<-dataMatrix[i,]+rep(c(0,3),each=5)
  }
}
#plot pattern data
par(mar=rep(0.2,4))
image(1:10,1:40,t(dataMatrix)[,nrow(dataMatrix):1])

#What if we add a pattern? the clustered data
par(mar=rep(0.2,4))
heatmap(dataMatrix)

#Patterns in rows and columns
hh <-hclust(dist(dataMatrix))
dataMatrixOrdered <-dataMatrix[hh$order,]
par(mfrow=c(1,3),mar=c(4,4,2,1),oma=c(0,0,2,0))
image(t(dataMatrixOrdered)[,nrow(dataMatrixOrdered):1])
plot(rowMeans(dataMatrixOrdered),40:1,xlab="Row Mean",ylab="Row",pch=20)
plot(colMeans(dataMatrixOrdered),xlab="column",ylab="Column Mean",pch=20)

```

##Dimension Reduction(part2)

```{r,echo=TRUE,message=FALSE,warning=FALSE}
#components of the svd (u and v)
dataMatrix <-matrix(rnorm(400),nrow=40)
hh <-hclust(dist(dataMatrix))
dataMatrixOrdered <-dataMatrix[hh$order,]
svd1 <-svd(scale(dataMatrixOrdered))
par(mfrow=c(1,3),mar=c(4,4,2,1),oma=c(0,0,2,0))
image(t(dataMatrixOrdered)[,nrow(dataMatrixOrdered):1])
plot(svd1$u[,1],40:1,xlab="Row",ylab="First left Singular Vector",pch=20)
plot(svd1$v[,1],xlab="Column",ylab="First right Singular Vector",pch=20)

#Relationship to principal components analysis
pca1 <-prcomp(dataMatrixOrdered,scale=TRUE)
plot(pca1$rotation[,1],svd1$v[,1],pch=20,xlab="Principal Component 1",ylab="Right Singular Vector 1")
abline(c(0,1))
#SVD and PCA are essentially the same thing.

```
