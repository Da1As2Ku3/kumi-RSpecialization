---
title: "Multivariable Regression"
author: "David Asare Kumi"
date: "June 22, 2019"
output: html_document
---

## Multivariable Regression

- This is an extension of linear regression where more independent variables are added to the model.

- The general linear model extends simple linear regression (SLR) by adding terms linearly into the model.

## How to get estimates

### Example

```{r,echo=TRUE,message=FALSE,warning=FALSE,comment=""}
n<-100
x<-rnorm(n)
x2<-rnorm(n)
x3<-rnorm(n)
y<-1+x+x2+x3+rnorm(n,sd=0.1) #mean=0 (origin)
ey<-resid(lm(y~x2+x3))
ex<-resid(lm(x~x2+x3))
#regression to the origin
sum(ey*ex)/sum(ex^2)
coef(lm(y~x+x2+x3))
#alternatively
coef(lm(ey~ex-1)) # take out the intercept

```

## Interpretation of the coefficients

- The interpretation of a multivariable regression coefficient is the expected change in the response variable per unit change in the regressor variable holding all of the other regressors fixed(constant).

## Multivariable Regression tips and tricks

### Example (part1)

```{r,echo=TRUE,message=FALSE,warning=FALSE,fig.width=10,comment=""}
library(datasets)
data(swiss)
library(ggplot2)
library(GGally)
g <-ggpairs(swiss,1:6,lower=list(continuous=wrap(ggally_points,size=1,color="red")))
g
#The GGally package gives you the scatterplot,density plots and the correlations of the variables.

#Plot densities of the variables
par(mfrow=c(2,3),mar=c(5,5,4,1),oma=c(0,0,4,0))
plot(density(swiss$Fertility),col="red",lty=1,lwd=2)
plot(density(swiss$Agriculture),col="red",lty=1,lwd=2)
plot(density(swiss$Examination),col="red",lty=1,lwd=2)
plot(density(swiss$Education),col="red",lty=1,lwd=2)
plot(density(swiss$Catholic),col="red",lty=1,lwd=2)
plot(density(swiss$Infant.Mortality),col="red",lty=1,lwd=2)

#Calculation of correlations
cor(swiss)

```

## Multiple Regression Coefficients

```{r,echo=TRUE,message=FALSE,warning=FALSE,comment=""}
library(datasets)
data(swiss)
library(ggplot2)
fit <-lm(Fertility~.,data=swiss)
#Various ways to derive regression coefficients
fit
coef(fit)
summary(fit)$coef
summary(fit)

```

## Interpretation of Regression coefficients

- Intercept; We expect 66.19 standardized fertility when all the other variable are zero (0).

- Agriculture; We expect a 0.17 decrease in standardized fertility for every 1% increase in percentage of males involved in agriculture holding the remaining variables constant.

- Examination; We expect a 0.25 decrease in standardized fertility for every 1% increase in percentage of draftees receiving highest mark on army examinaition holding the remaining variables constant.

- Education; We expect a 0.87 decrease in standardized fertility for every 1% increase in percentage of education beyond primary school for draftees holding the remaining variables constant.

- Catholic; we expect a 0.10 increase in standardized fertility for every 1% increase in percentage of catholic as opposed to protestant holding the remaining variables constant.

- Infant.Mortality; We expect a 1.07 increase in standardized fertility for every increase in infant mortality live births who live less than 1 year.


## Model Selection

```{r,echo=TRUE,message=FALSE,warning=FALSE,comment=""}
library(datasets)
data(swiss)
library(ggplot2)
fit <-lm(Fertility~Agriculture,data=swiss)
summary(fit)$coef
#Scatterplot
g <-ggplot(swiss,aes(x=Agriculture,y=Fertility))+geom_point(size=2,color="red",alpha=1)
g <-g+geom_smooth(method="lm",color="blue",size=2,se=FALSE)
g <-g+labs(x="Agricultue",y="Fertility")+labs(title="Scatterplot of Agriculture and Fertility")
g

```

- The agriculture variable has a positive coefficient now (i.e. 0.1942). It is important to note that adjusting for the other variables changes the actual direction of the effect of agriculture on fertility. This is called Simpson's paradox (Simpson's perceived paradox).

- The sign reverses itself with the inclusion of Examination and Education.

- The percent of males in the province working in Agriculture is negatively related to educational attainment (correlation of -0.6395) and Education and Examination (correlation of 0.6984) are obviously measuring similar things.

- At the minimum, everyone claiming that provinces that are more agriculture inclined have higher fertility rates would immediately be open to criticisms.

- If Z=swiss$Agriculture + swiss$Education, then lm(Fertility~.+Z,data=swiss) has no added value because we already have the variables in the model. The Z variable created is assigned NA (totally unnecessary). Z is numerically related to other variables in the model already that is why it was assigned NA.

## Multivariable Regression

### Example part II (Factor Variables)

```{r,echo=TRUE,message=FALSE,warning=FALSE,comment=""}
require(datasets)
require(stats)
require(ggplot2)
data(InsectSprays)
dim(InsectSprays)
str(InsectSprays)
summary(InsectSprays)
g <-ggplot(data=InsectSprays,aes(y=count,x=spray,fill=spray))
g <-g + geom_violin(color="black",size=2)
g <-g + xlab("Type of Spray") + ylab("Insect Count")
g

#Fit a model
fit <-lm(count~spray,data=InsectSprays)
summary(fit)$coef

```

- Spray A is missing - this means that everything here is in comparison with spray A. 0.833 is the change in the mean between spray B and spray A.

- The intercept 14.5 is the mean for spray A.

- The mean for spray B is 14.5 + 0.833 = 15.333.

- The mean for spray C is 14.5 - 12.417 = 2.083.

- The mean for spray D is 14.5 - 9.58 = 4.92.

- The mean for spray E is 14.5 - 11.0 = 3.50.

- The mean for spray F is 14.5 + 2.17 = 16.67.

- Compare spray B and C ; 0.833 - (-12.417) = 0.833 + 12.417 = 13.250.


## Hard coding the dummy variable 

- Here we do not allow R to do the reference.

```{r,echo=TRUE,message=FALSE,warning=FALSE,comment=""}
require(datasets)
require(stats)
require(ggplot2)
data(InsectSprays)
dim(InsectSprays)
fit <-lm(count~I(1*(spray=="B"))+I(1*(spray=="C"))+I(1*(spray=="D"))+I(1*(spray=="E"))+I(1*(spray=="F")),data=InsectSprays)
summary(fit)$coef

```

- lm(count~spray) ;This picks the lowest alphanumeric variable and sets it as a reference.

- The hard coding approach also sets spray A as the reference and the results is the same as the one obtained.

- What if we include all six(6) sprays i.e. spray A is added; when you include spray A, the lm model returns NA for spray A because it is redundant.

## What if you want the coefficients to be the mean for the group

- You can do this but you have to remove the intercept by subtracting 1.

```{r,echo=TRUE,message=FALSE,warning=FALSE,comment=""}
require(datasets)
require(stats)
require(ggplot2)
data(InsectSprays)
fit <-lm(count~spray-1,data=InsectSprays)
summary(fit)$coef

#Calculate the means to cross check with the coefficients
library(dplyr)
summarise(group_by(InsectSprays,spray),mn=mean(count))
#This model is the same as the one with the intercept.

```

## Reordering the levels

```{r,echo=TRUE,message=FALSE,warning=FALSE,comment=""}
require(datasets)
require(stats)
require(ggplot2)
data(InsectSprays)
spray2 <-relevel(InsectSprays$spray,"C")
fit <-lm(count~spray2,data=InsectSprays)
summary(fit)$coef
#The intercept is interpreted as the mean of the reference spray.

```

## Summary

- If we treat spray as a factor, R includes an intercept and omits the alphabetically first level of the factor.

- All t tests are for comparisons of Sprays versus Spray A.

- Empirical mean for A is the intercept.

- Other group means are the intercepts plus their coefficients.

- If we omit an intercept(itc) then it includes terms for all levels of Spray.

1. Group means are the coefficients.

2. Tests are tests of whether the groups are different than zero.

- If we want comparisons between Spray B and C, say relevel so that the B is the reference level.


## Multivariable Regression (Example Part III)

### Analysis of Covariates (ANCOVA)

```{r,echo=TRUE,message=FALSE,warning=FALSE,comment=""}
library(datasets)
library(dplyr)
library(ggplot2)
data(swiss)
names(swiss)
#since its majority catholic let's use dplyr to create a factor variable (binary)
swiss <-mutate(swiss,CatholicBin=1*(Catholic >50))
#Plot the data
g <-ggplot(swiss,aes(x=Agriculture,y=Fertility))+ geom_point(size=2,color="black")
g <-g + labs(x="% in Agriculture",y="Fertility")+labs(title="Scatterplot of Agriculture and Fertility")
g
#Fit model that does not include religion at all
fit <-lm(Fertility~Agriculture,data=swiss)
g1 <-g
g1 <-g1 + geom_abline(intercept=coef(fit)[1],slope=coef(fit)[2],size=2)
g1
summary(fit)$coef
#Let's fit two parallel lines
fit <-lm(Fertility~Agriculture+factor(CatholicBin),data=swiss)
g1 <-g
g1 <-g1 + geom_abline(intercept=coef(fit)[1],slope=coef(fit)[2],size=2)
g1 <-g1+geom_abline(intercept=coef(fit)[1]+coef(fit)[3],slope=coef(fit)[2],size=2,color="red")
g1
summary(fit)$coef
#Lines where there are different intercepts and diffferent slopes
fit <-lm(Fertility~Agriculture*factor(CatholicBin),data=swiss)
summary(fit)$coef
#Now let's plot the two lines
g1 <-g
g1 <-g1 + geom_abline(intercept=coef(fit)[1],slope=coef(fit)[2],size=2,color="red")
g1 <-g1 + geom_abline(intercept=coef(fit)[1]+coef(fit)[3],slope=coef(fit)[2]+coef(fit)[4],size=2)
g1

```

## Adjustment

- Here we will use simulation to investigate how adding a regressor in to a model addresses the idea of adjustment.

- Things can change to exact opposite if you perform adjustment.


## Example

```{r,echo=TRUE,message=FALSE,warning=FALSE,comment=""}
n <-100
t <-rep(c(0,1),c(n/2,n/2))
x <-c(runif(n/2),runif(n/2))
beta0 <-0
beta1 <-1
tau <-1
sigma <-0.2
y <-beta0+x*beta1+t*tau+rnorm(n,sd=sigma)
plot(x,y,type="n",frame=FALSE)
abline(h=mean(y[1:(n/2)]),lwd=3)
abline(h=mean(y[(n/2+1):n],lwd=3))
fit <-lm(y~x+t)
abline(coef(fit)[1],coef(fit)[2],lwd=3)
abline(coef(fit)[1]+coef(fit)[3],coef(fit)[2],lwd=3)
points(x[1:(n/2)],y[1:(n/2)],pch=20,col="black",bg="lightblue",cex=2)
points(x[(n/2+1):n],y[(n/2+1):n],pch=20,col="black",bg="salmon",cex=2)

```

## Residuals Again

- The vertical distances between the observed data points and the fitted regression line are called residuals.We can generalize this idea to the vertical distances between the observed data and the fitted surface in multivariable settings.


## Residuals and Diagnostics (Part I)

```{r,echo=TRUE,message=FALSE,warning=FALSE,fig.width=8,comment=""}
library(datasets)
library(ggplot2)
data(swiss)
par(mfrow=c(2,2),mar=c(4,4,2,1),oma=c(0,0,2,0))
fit <-lm(Fertility~.,data=swiss)
summary(fit)$coef
plot(fit,cex=1,pch=20)

```

## Influencial high leverage and outlying points

- Leverag; How far away from the center of the axis the data point is.

- Influence: Whether or not that point chooses to exert that leverage.

## Residuals and Diagnostics (Part II)

### Summary of the plot

- Calling a point an outlier is vague. Outliers can be the results of spurious or real processes.

- Outliers can conform to the regression relationship (i.e. being marginally outlying in X or y, but not outlying given the regression relationship).


## Influence Measures

- Do ?influence.measures to see the full suite of influence measures in stats. The measures include;

1. rstandard; Standardized residuals, residuals divided by their standard deviations.

2. rstudent; Standardized residuals, residuals divided by their estimated standard deviations.

- hatvalues; measures of leverage. They are useful for finding data entry errors.

- dffits;Proposed by Welsch and Kuh (1977). It is the scaled difference between the ith fitted value obtained from the full data and the ith fitted value obtained by deleting the ith observation. DFFIT - difference in fits, is used to identify influential data points. It quantifies the number of standard deviations that the fitted value changes when the ith data point is omitted.
An observation is deemed influential if the absolute value of its DFFITS value is greater than:

               2*√(p+1)/(n−p−1)

where n is the number of observations and p is the number of predictors including intercept.
To plot dffits;
model <- lm(mpg ~ disp + hp + wt + qsec, data = mtcars)
ols_plot_dffits(model)

- DFBETA measures the difference in each parameter estimate with and without the influential point. There is a DFBETA for each data point i.e if there are n observations and k variables, there will be n∗k DFBETAs. In general, large values of DFBETAS indicate observations that are influential in estimating a given parameter. Belsley, Kuh, and Welsch recommend 2 as a general cutoff value to indicate influential observations and 2/√n as a size-adjusted cutoff.
To plot dfbetas panel;
model <- lm(mpg ~ disp + hp + wt, data = mtcars)
ols_plot_dfbetas(model)

- Cook's Distance; Bar Plot of Cook’s distance to detect observations that strongly influence fitted values of the model. Cook’s distance was introduced by American statistician R Dennis Cook in 1977. It is used to identify influential data points. It depends on both the residual and leverage i.e it takes into account both the x value and y value of the observation.
A data point having a large cook’s d indicates that the data point strongly influences the fitted values.
Cook’s D Bar Plot
model <- lm(mpg ~ disp + hp + wt + qsec, data = mtcars)
ols_plot_cooksd_bar(model)
Cook’s D Chart
Chart of Cook’s distance to detect observations that strongly influence fitted values of the model.

model <- lm(mpg ~ disp + hp + wt + qsec, data = mtcars)
ols_plot_cooksd_chart(model)

- resid; returns the ordinary residuals.

- PRESS residual = resid(fit)/(1-hatvalues(fit)) where fit is the fitted regression line.

## Problems of residuals

- Residuals are unit specific. If your y is in $ then the residuals must be in $.

- They are not necessarily compared across settings.

## Some influence plots

```{r,echo=TRUE,message=FALSE,warning=FALSE,comment=""}
library(datasets)
library(ggplot2)
library(olsrr)
data(swiss)
fit <-lm(Fertility~.,data=swiss)
#Cook’s D Bar Plot
ols_plot_cooksd_bar(fit)
#Cook’s D Chart
ols_plot_cooksd_chart(fit)
#dfbetas panel;
ols_plot_dfbetas(fit)
#plot dffits
ols_plot_dffits(fit)

```

## Residuals and Diagnostics (Part III)

### Case 1

```{r,echo=TRUE,message=FALSE,warning=FALSE,comment=""}
n <-100
x <-c(10,rnorm(n))
y <-c(10,c(rnorm(n)))
plot(x,y,cex=2,pch=21,bg="lightblue",col="black",frame=FALSE)
fit <-lm(y~x)
abline(fit,lwd=2)
#showing a couple of the diagnostic values
round(dfbetas(fit)[1:10,2],3)
#dfbetas value of 6.317 is the outlying point.
round(hatvalues(fit)[1:10],3)
#hatvalues has to be between 0 and 1

#Diagnostic plots
par(mfrow=c(2,2),mar=c(4,4,2,1),oma=c(0,0,2,0))
plot(fit)

```

## Poisson Regression

- Number of calls to a call center.

- Number of hits to a website.

- Number of flu cases in an area.

- The starting point for most count analysis is the Poisson Distribution.

## Poisson Regression (Part I)

### Key ideas

- Many data take the form of counts.

A. Calls to a call center.

B. Number of flu cases in an area.

C. Number of cars that cross a bridge.

-  Data may also be in the form of rates

A. Percent of children passing a test.

B. Percent of hits to a website from a country.

- Linear regression with transformation is an option.

NB: In each of these cases the counts are unbounded.


## Poisson Distribution

- The poisson distribution is a useful model for counts and rates.

- Here a rate is count per some monitoring time.

- Some examples - uses of the poisson distribution.

1. Modelling web traffic hits.

2. Incidence rates.

3. Approximating binomial probabilities with small p and large n.

4. Analyzing contingency table data.

- The mean of the poisson is lambda and the variance is lambda i.e. mean = variance = lambda.

- The poisson turns to a normal as the mean becomes larger.

## Example of simulation


```{r,echo=TRUE,message=FALSE,warning=FALSE,comment=""}
par(mfrow=c(1,3),mar=c(4,4,2,1),oma=c(0,0,2,0))
plot(0:10,dpois(0:10,lambda=2),type="h",main="Mean=2",frame=FALSE)
plot(0:20,dpois(0:20,lambda=10),type="h",main="Mean=10",frame=FALSE)
plot(0:200,dpois(0:200,lambda=100),type="h",main="Mean=100",frame=FALSE)
#You can specify type="l", that gives a line.
par(mfrow=c(1,3),mar=c(4,4,2,1),oma=c(0,0,2,0))
plot(0:10,dpois(0:10,lambda=2),type="l",main="Mean=2",frame=FALSE)
plot(0:20,dpois(0:20,lambda=10),type="l",main="Mean=10",frame=FALSE)
plot(0:200,dpois(0:200,lambda=100),type="l",main="Mean=100",frame=FALSE)

```

## Show that the mean and variance of the poisson are equal.

```{r,echo=TRUE,message=FALSE,warning=FALSE,comment=""}
x <-0:10000
lambda <-3
mu <-sum(x*dpois(x,lambda=lambda))
sigmasq <-sum((x-mu)^2*dpois(x,lambda=lambda))
c(mu,sigmasq)

```

- Geometric mean is just exponentiating the arithmetic mean.

## How to fit functions using linear models

### Simulated Example

```{r,echo=TRUE,message=FALSE,warning=FALSE,comment=""}
n <-500
x <-seq(0,4*pi,length=n)
y <-sin(x)+rnorm(n,sd=0.3)
knots <-seq(0,8*pi,length=20)
splineTerms <-sapply(knots,function(knot)(x>knot)*(x-knot))
xMat <-cbind(1,x,splineTerms)
yhat <-predict(lm(y~xMat-1))
plot(x,y,frame=FALSE,pch=21,bg="lightblue",cex=2)
lines(x,yhat,col="red",lwd=2)

```

## Adding squared terms

- Adding squared terms makes it continuously differentiable at the knot points.

- Adding cubic terms makes it twice continuously differentiable at the knot points etcetera.

### The code is identical

```{r,echo=TRUE,message=FALSE,warning=FALSE,comment=""}
n <-500
x <-seq(0,4*pi,length=n)
y <-sin(x)+rnorm(n,sd=0.3)
knots <-seq(0,8*pi,length=20)
splineTerms <-sapply(knots,function(knot)(x>knot)*(x-knot)^2)
xMat <-cbind(1,x,x^2,splineTerms)
yhat <-predict(lm(y~xMat-1))
plot(x,y,frame=FALSE,pch=21,bg="lightblue",cex=2)
lines(x,yhat,col="red",lwd=2)

```

## Notes - Regression Splines

- The collection of regressors is called a basis (building blocks).

- People have spent a lot of time thinking about basis for this kind of problem. So consider this as just a teaser.

- Single knot point terms can fit hockey stick like processes.

- These bases can be used in GLMs as well.

- An issue with this approach is the large number of parameters introduced. Requires some method of so called regularization.

- Problem; fit in lots and lots of knot points.

## Harmonics using linear models

### Chord finder, playing the white keys on a piano from octave C4 - c5.

```{r,echo=TRUE,message=FALSE,warning=FALSE,fig.width=8,comment=""}
notes4 <-c(261.63,293.66,329.63,349.23,392.00,440.00,493.88,523.25)
t <-seq(0,2,by=0.001)
n <-length(t)
c4 <-sin(2*pi*notes4[1]*t)
e4 <-sin(2*pi*notes4[3]*t)
g4 <-sin(2*pi*notes4[5]*t)
chord <-c4+e4+g4+rnorm(n,0,0.3)
x <-sapply(notes4,function(freq)sin(2*pi*freq*t))
fit <-lm(chord~x-1)
fit
plot(chord,type="l")
#Diagnostic plots
par(mfrow=c(2,2),mar=c(4,4,2,1))
plot(fit,cex=1,pch=20)

#Discrete Fourier Transform
a <-fft(chord)
plot(Re(a)^2,type="l")

```
